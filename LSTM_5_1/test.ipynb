{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1027\n"
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def one_hot(x, n_class, dtype=torch.float32): \n",
    "    # X shape: (batch), output shape: (batch, n_class)\n",
    "    x = x.long()\n",
    "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n",
    "    res.scatter_(1, x.view(-1, 1), 1)\n",
    "    return res\n",
    "\n",
    "x = torch.tensor([0, 2])\n",
    "one_hot(x, vocab_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5 torch.Size([2, 1027])\n[tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])]\n"
    }
   ],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def to_onehot(X, n_class):  \n",
    "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\n",
    "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n",
    "\n",
    "X = torch.arange(10).view(2, 5)\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "will use cuda\n"
    }
   ],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "print('will use', device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5 torch.Size([2, 1027]) torch.Size([2, 256])\n"
    }
   ],
   "source": [
    "state = init_rnn_state(X.shape[0], num_hiddens, device)\n",
    "inputs = to_onehot(X.to(device), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens, device)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n",
    "        print(X)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'分开断初邂手承己简野器看'"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            device, idx_to_char, char_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "            else:  \n",
    "            # 否则需要使用detach函数从计算图分离隐藏状态, 这是为了\n",
    "            # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "\n",
    "            inputs = to_onehot(X, vocab_size)\n",
    "            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "            (outputs, state) = rnn(inputs, state, params)\n",
    "            # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "            # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n",
    "            # 使用交叉熵损失计算平均分类误差\n",
    "            l = loss(outputs, y.long())\n",
    "\n",
    "            # 梯度清0\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n",
    "            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "uda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n - 分开 有一个人废 你在它停留的 为什么我女朋友场外加油 你却还让我出糗 从小就耳了目染 什么刀枪跟棍棒 \n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n - 不分开想 我不能再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\nepoch 200, perplexity 1.584389, time 0.33 sec\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n - 分开 沙天去空 全暖了空 盲敢不发 你一定 连一句珍重 也有苦衷 全候怕日出 白色蜡烛 温暖了空屋 白色\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n - 不分开吗 我叫你爸 你打我妈 这样对吗干嘛这样 何必让酒重鼻子走 瞎 我不你有想要 不发再这样不要 我后悔\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\nepoch 250, perplexity 1.330595, time 0.34 sec\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n - 分开 出前在 干什么 我有就这样牵着你的手不放开 爱可不能够永远单纯没有悲哀 我 想带你骑单车 我 想和\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n[tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')]\n - 不分开吗把的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 一只海  疗人梦空 恨再种从 没\n"
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = d2l.data_iter_random(corpus_indices, batch_size, num_steps, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 261.,  261.,  263.,  ..., 1022.,  804.,  152.],\n        [ 566., 1009.,  245.,  ...,  631.,  448.,  324.],\n        [ 971.,  190.,  575.,  ...,    0.,  324.,  577.],\n        ...,\n        [ 591.,  454., 1022.,  ...,  448.,  988., 1022.],\n        [ 248., 1008., 1022.,  ...,  264., 1022.,  674.],\n        [ 423.,  324.,  385.,  ...,  927.,   66.,  168.]], device='cuda:0')\ntorch.Size([32, 35])\n<class 'torch.Tensor'>\n<class 'list'>\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], device='cuda:0')\ntorch.Size([32, 10])\n"
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X)\n",
    "    print(X.shape)\n",
    "    print(type(X))\n",
    "    inputs = to_onehot(X, 10)\n",
    "    print(type(inputs))\n",
    "    print(inputs[0])\n",
    "    print(inputs[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-60b5818eb127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstmlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[0mbatch_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m             \u001b[0mmax_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[0msorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.arange(24).reshape(2, 3, 4)\n",
    "l = nn.LSTM(input_size=4, hidden_size=5, num_layers=2)\n",
    "state = None\n",
    "for _ in range(10000):\n",
    "    y, state = l(x.float(), state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 3, 5])\n"
    }
   ],
   "source": [
    "print(state[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 8.8804e-02, -8.5487e-03,  9.2469e-02,  1.9737e-01, -6.2592e-02],\n         [ 8.0306e-02, -5.2223e-05,  1.2938e-02,  1.1293e-01, -1.2486e-02],\n         [ 4.9383e-02, -2.0724e-07,  1.7562e-03,  5.7833e-02, -2.0174e-03]],\n\n        [[ 9.5629e-02,  4.8861e-02, -8.6619e-02, -2.5372e-02,  2.1580e-01],\n         [ 8.8721e-02,  5.2430e-02, -9.0381e-02, -3.7910e-02,  2.2498e-01],\n         [ 8.4213e-02,  5.1157e-02, -8.6353e-02, -4.3962e-02,  2.3711e-01]]],\n       grad_fn=<StackBackward>)\n"
    }
   ],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    " y = l(x.float(), (h,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[[ 0.1303, -0.0056,  0.0333, -0.2250,  0.0704],\n          [ 0.1171, -0.0193,  0.0474, -0.2487,  0.0881]],\n \n         [[ 0.0634, -0.0803,  0.0697, -0.2896,  0.1051],\n          [ 0.0673, -0.0893,  0.0781, -0.3115,  0.1183]],\n \n         [[ 0.0632, -0.0837,  0.0715, -0.2994,  0.1142],\n          [ 0.0706, -0.0890,  0.0781, -0.3177,  0.1271]]],\n        grad_fn=<TransposeBackward0>),\n (tensor([[[-1.6319e-02,  1.0577e-01, -5.4034e-01,  5.3460e-02, -6.4752e-02],\n           [-4.3859e-06,  6.9505e-02, -7.2742e-01,  5.7375e-03,  1.0671e-01],\n           [-3.7784e-10,  1.2939e-02, -7.5732e-01,  7.7466e-04,  3.7851e-02]],\n  \n          [[ 1.1706e-01, -1.9299e-02,  4.7393e-02, -2.4871e-01,  8.8145e-02],\n           [ 6.7296e-02, -8.9316e-02,  7.8069e-02, -3.1147e-01,  1.1830e-01],\n           [ 7.0560e-02, -8.8997e-02,  7.8114e-02, -3.1775e-01,  1.2706e-01]]],\n         grad_fn=<StackBackward>),\n  tensor([[[-1.6858e-02,  1.1098e-01, -8.7103e-01,  5.6414e-02, -6.6475e-02],\n           [-4.3867e-06,  6.9651e-02, -9.8785e-01,  5.7389e-03,  1.0713e-01],\n           [-3.7784e-10,  1.2940e-02, -9.9915e-01,  7.7466e-04,  3.7869e-02]],\n  \n          [[ 1.9994e-01, -4.8026e-02,  1.2526e-01, -5.0906e-01,  2.6010e-01],\n           [ 1.1674e-01, -2.1690e-01,  2.0440e-01, -6.2517e-01,  3.8532e-01],\n           [ 1.2164e-01, -2.2016e-01,  2.0372e-01, -6.2329e-01,  4.1614e-01]]],\n         grad_fn=<StackBackward>)))"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "  import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import torchtext.vocab as Vocab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def mySample(data):\n",
    "    num = 0\n",
    "    total = len(train_set)\n",
    "    sample_list = []\n",
    "\n",
    "    for X, y in train_set:\n",
    "        num = num + y\n",
    "    \n",
    "\n",
    "    class_sample_count = np.array([total - num, num])\n",
    "    weight = 1. / class_sample_count\n",
    "\n",
    "    for X, y in train_set:\n",
    "        sample_list.append(weight[int(y)])\n",
    "\n",
    "    sample_weight = torch.FloatTensor(sample_list)\n",
    "    sampler = WeightedRandomSampler(sample_weight, total)\n",
    "    return sampler\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, traindata, numstep, length):\n",
    "        self.serial_number = traindata['serial_number'].value_counts()\n",
    "        self.value = traindata.iloc[:, length:-1].values\n",
    "        max = np.max(self.value)\n",
    "        min = np.min(self.value)\n",
    "        scalar = max - min \n",
    "        self.datas = list(map(lambda x: x / scalar, self.value))\n",
    "        self.datalabel = train_data.loc[:, 'label'].values.tolist()\n",
    "        self.input = []\n",
    "        self.label = []\n",
    "\n",
    "        for diskname in self.serial_number.index.sort_values():\n",
    "            traindata_name = traindata[traindata.serial_number == diskname]\n",
    "            self.datalabel = traindata_name.loc[:, 'label'].values.tolist()\n",
    "            for i in range(len(traindata_name) - numstep):\n",
    "                self.input.append(torch.Tensor(self.datas[i: i + numstep]))\n",
    "                if(torch.Tensor(self.datalabel[i: i + numstep]).sum() != 0):\n",
    "                    self.label.append(torch.ones(1))\n",
    "                else:\n",
    "                    self.label.append(torch.zeros(1))\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def __getitem__(self, idx):\n",
    "        #data = self.datas[idx].reshape(1, -1)\n",
    "        return self.input[idx], self.label[idx]\n",
    "\n",
    "train_data = pd.read_csv('E:/LSTM_5_1/train_2018_1_model_2.csv')\n",
    "train_set = MyData(train_data, 7, 5)\n",
    "sampler = mySample(train_set)\n",
    "train_iter = DataLoader(MyData(train_data, 7, 5), batch_size = 10, sampler=sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(in_channels = 13, out_channels = 128, kernel_size = 4) # 要求转置\n",
    "lstmlayer = nn.LSTM(input_size=128, hidden_size=128, num_layers=2)\n",
    "linear = nn.Linear(128, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\ntorch.Size([10, 7, 128])\n"
    }
   ],
   "source": [
    "state = None\n",
    "for X, y in train_iter:\n",
    "    pad = nn.ZeroPad2d(padding=(2, 1, 0, 0))\n",
    "    X = X.permute(0, 2, 1)\n",
    "    X = pad(X)\n",
    "    X = conv1(X)\n",
    "    X =  X.permute(0, 2, 1) \n",
    "    print(X.shape)\n",
    "    X, state = lstmlayer(X, state)\n",
    "    X = X[:, X.size(1) - 1, :]\n",
    "    x = torch.sigmoid(X)\n",
    "    X = linear(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0696, -0.0131],\n        [-0.0707, -0.0133],\n        [-0.0714, -0.0135],\n        [-0.0719, -0.0136],\n        [-0.0713, -0.0137],\n        [-0.0717, -0.0136],\n        [-0.0729, -0.0145]], grad_fn=<AddmmBackward>)"
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([7, 2])"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([10, 7, 13])\ntorch.Size([7, 7, 13])\n"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "for X, y in train_iter:\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import torchtext.vocab as Vocab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def mySample(data):\n",
    "    num = 0\n",
    "    total = len(train_set)\n",
    "    sample_list = []\n",
    "\n",
    "    for X, y in train_set:\n",
    "        num = num + y\n",
    "    \n",
    "\n",
    "    class_sample_count = np.array([total - num, num])\n",
    "    weight = 1. / class_sample_count\n",
    "\n",
    "    for X, y in train_set:\n",
    "        sample_list.append(weight[int(y)])\n",
    "\n",
    "    sample_weight = torch.FloatTensor(sample_list)\n",
    "    sampler = WeightedRandomSampler(sample_weight, total)\n",
    "    return sampler\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, traindata, numstep, length):\n",
    "        self.serial_number = traindata['serial_number'].value_counts()\n",
    "        self.value = traindata.iloc[:, length:-1].values\n",
    "        max = np.max(self.value)\n",
    "        min = np.min(self.value)\n",
    "        scalar = max - min \n",
    "        self.datas = list(map(lambda x: x / scalar, self.value))\n",
    "        self.datalabel = train_data.loc[:, 'label'].values.tolist()\n",
    "        self.input = []\n",
    "        self.label = []\n",
    "\n",
    "        for diskname in self.serial_number.index.sort_values():\n",
    "            traindata_name = traindata[traindata.serial_number == diskname]\n",
    "            self.datalabel = traindata_name.loc[:, 'label'].values.tolist()\n",
    "            for i in range(len(traindata_name) - numstep):\n",
    "                self.input.append(torch.Tensor(self.datas[i: i + numstep]))\n",
    "                if(torch.Tensor(self.datalabel[i: i + numstep]).sum() != 0):\n",
    "                    self.label.append(torch.ones(1))\n",
    "                else:\n",
    "                    self.label.append(torch.zeros(1))\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def __getitem__(self, idx):\n",
    "        #data = self.datas[idx].reshape(1, -1)\n",
    "        return self.input[idx], self.label[idx]\n",
    "\n",
    "train_data = pd.read_csv('E:/LSTM_5_1/train_2018_1_model_2.csv')\n",
    "train_set = MyData(train_data, 7, 5)\n",
    "sampler = mySample(train_set)\n",
    "train_iter = DataLoader(MyData(train_data, 7, 5), batch_size = 10, sampler=sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3659 3628\n3674 3613\n3602 3685\n3602 3685\n3625 3662\n3566 3721\n3702 3585\n3728 3559\n3651 3636\n3567 3720\n"
    }
   ],
   "source": [
    "p = 0\n",
    "n = 0\n",
    "for i in range(10):\n",
    "    p = 0\n",
    "    n = 0\n",
    "    for X, y in train_iter:\n",
    "        if(y.sum() == 1):\n",
    "            p = p + 1\n",
    "        if(y.sum() == 0):\n",
    "            n = n + 1\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        assert len(input_seq.size()) > 2\n",
    "\n",
    "        # reshape input data --> (samples * timesteps, input_size)\n",
    "        # squash timesteps\n",
    "        reshaped_input = input_seq.contiguous().view(-1, input_seq.size(-1))\n",
    "\n",
    "        output = self.module(reshaped_input)\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            # (samples, timesteps, output_size)\n",
    "            output = output.contiguous().view(input_seq.size(0), -1, output.size(-1))\n",
    "        else:\n",
    "            # (timesteps, samples, output_size)\n",
    "            output = output.contiguous().view(-1, input_seq.size(1), output.size(-1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(in_channels = 13, out_channels = 128, kernel_size = 4)\n",
    "Time = TimeDistributed(conv1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10, 7, 13])\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [128, 13, 4], but got 2-dimensional input of size [70, 13] instead",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-15fe10fd2a32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-dc32951e5c2a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_seq)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mreshaped_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreshaped_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m# We have to reshape Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    205\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 207\u001b[1;33m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    208\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [128, 13, 4], but got 2-dimensional input of size [70, 13] instead"
     ]
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print(X.shape)\n",
    "    output = Time(X)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "disk = pd.read_csv('E:/LSTM_5_1/train_2018_1_model_2.csv')\n",
    "data = disk.iloc[:, 5:-1].values\n",
    "max = np.max(data)\n",
    "min = np.min(data)\n",
    "scalar = max - min\n",
    "data_scalar = list(map(lambda x: x / scalar, data))\n",
    "targets = disk.loc[:, 'label']\n",
    "tragets = np.array(targets)\n",
    "\n",
    "data_gen = TimeseriesGenerator(data_scalar, targets,\n",
    "                               length=10, sampling_rate=1,\n",
    "                               batch_size=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(train_X, train_y, test_X, test_y):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # model.add(TimeDistributed(cnn))\n",
    "    model.add(TimeDistributed(Convolution1D(128, 4, border_mode='same'), input_shape=train_X.shape[1:]))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(128, return_sequences=True, name=\"lstm_layer0\")) #output = batch * step * output_size\n",
    "    model.add(LSTM(128, return_sequences=False, name=\"lstm_layer1\")) #output = batch * output_size(last step of every batch)\n",
    "    # model.add(LSTM(100, return_sequences=True, name=\"lstm_layer2\"))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    # model.add(GlobalAveragePooling1D(name=\"global_avg\"))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_X, train_y,batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, validation_data=(test_X, test_y))\n",
    "\n",
    "    test_y = test_y.reshape(test_y.size, 1)\n",
    "    predict_y = model.predict(test_X)\n",
    "    predict_y = predict_y.reshape(predict_y.size, 1)\n",
    "    predict_y = classifyRes(predict_y)\n",
    "    calMetrix(__file__, predict_y, test_y)\n",
    "    t0 = time.strftime('%Y%m%d%H%M%S', time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Layer sequential_1 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ]],\n\n       ...,\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.70707071],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.75757576]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.70707071],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.75757576],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.76767677]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.75757576],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.76767677],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.78787879]]])]. All inputs to the layer should be tensors.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0m\u001b[0;32m    697\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-808276866331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvolution1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborder_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 raise ValueError('Layer ' + self.name + ' was called with '\n\u001b[0m\u001b[0;32m    313\u001b[0m                                  \u001b[1;34m'an input that isn\\'t a symbolic tensor. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                                  \u001b[1;34m'Received type: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer sequential_1 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ]],\n\n       ...,\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.70707071],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.75757576]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.70707071],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.75757576],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.76767677]],\n\n       [[1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.85858586,\n         1.01010101, 0.6969697 ],\n        ...,\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.75757576],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.76767677],\n        [1.01010101, 1.01010101, 1.01010101, ..., 0.88888889,\n         1.01010101, 0.78787879]]])]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed #输入至少为3D张量\n",
    "# from matplotlib import pyplot as plt\n",
    "# import random\n",
    "import argparse\n",
    "import time\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "text_model = Sequential()\n",
    "text_model.add(Convolution1D(128, 4, border_mode='same'))\n",
    "for x, y in data_gen:\n",
    "    model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x, y = data_gen[-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[[ 0]\n  [ 1]\n  [ 2]\n  [ 3]\n  [ 4]\n  [ 5]\n  [ 6]\n  [ 7]\n  [ 8]\n  [ 9]]\n\n [[ 1]\n  [ 2]\n  [ 3]\n  [ 4]\n  [ 5]\n  [ 6]\n  [ 7]\n  [ 8]\n  [ 9]\n  [10]]]\n*************\n[[10]\n [11]]\n****************************************\n"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[i] for i in range(50)])\n",
    "targets = np.array([[i] for i in range(50)])\n",
    "\n",
    "data_gen = TimeseriesGenerator(data, targets,\n",
    "                               length=10, sampling_rate=1,\n",
    "                               batch_size=2)\n",
    "assert len(data_gen) == 20\n",
    "\n",
    "batch_0 = data_gen[0]\n",
    "x, y = batch_0\n",
    "\n",
    "for x, y in data_gen:\n",
    "    print(x)\n",
    "    print('*************')\n",
    "    print(y)\n",
    "    print('****************************************')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[10, 11, 12, 13, 14, 15, 16]"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "a  = []\n",
    "b = []\n",
    "for i in range(1000):\n",
    "    a.append(i)\n",
    "    b.append(i)\n",
    "a[10:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "18"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "b[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import torchtext.vocab as Vocab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = pd.read_csv('E:/LSTM_5_1/train_2018_1_model_2.csv')\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, data_train, numstep, length):\n",
    "        self.serial_number = data_train['serial_number'].value_counts()\n",
    "        self.value = data_train.iloc[:, length:-1].values\n",
    "        max = np.max(self.value)\n",
    "        min = np.min(self.value)\n",
    "        scalar = max - min \n",
    "        self.datas = list(map(lambda x: x / scalar, self.value))\n",
    "        self.datalabel = data_train.loc[:, 'label'].values.tolist()\n",
    "        self.input = []\n",
    "        self.label = []\n",
    "\n",
    "        for diskname in self.serial_number.index.sort_values():\n",
    "            traindata_name = data_train[data_train.serial_number == diskname]\n",
    "            self.datalabel = data_train.loc[:, 'label'].values.tolist()\n",
    "            for i in range(len(traindata_name) - numstep):\n",
    "                self.input.append(torch.Tensor(self.datas[i: i + numstep]))\n",
    "                if(self.datalabel[i + numstep - 1] != 0):\n",
    "                    self.label.append(torch.ones(1))\n",
    "                else:\n",
    "                    self.label.append(torch.zeros(1))\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def __getitem__(self, idx):\n",
    "        #data = self.datas[idx].reshape(1, -1)\n",
    "        return self.input[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "data_train = MyData(data, 7, 6)\n",
    "len(data_train)\n",
    "\n",
    "num = 0\n",
    "total = len(data_train)\n",
    "sample_list = []\n",
    "\n",
    "for X, y in data_train:\n",
    "    num = num + y\n",
    "\n",
    "    \n",
    "len(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitpytorchconda2fbdfbc796624a3f9e5f3a1ef21132b8",
   "display_name": "Python 3.8.3 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}